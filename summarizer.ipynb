{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"C://Users//saha//Downloads//fnana-15-712842.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(docs):\n",
    "    print(f'You have {len(docs)} documents')\n",
    "    nwords = sum([len(doc.page_content.split(' ')) for doc in docs])\n",
    "    print(f'You have {nwords} words')\n",
    "    print()\n",
    "    print(f'Preview: \\n{docs[0].page_content.split(\" \")[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm,chain_type='map_reduce',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_docs = txt_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 60 documents\n",
      "You have 7036 words\n",
      "\n",
      "Preview: \n",
      "METHODS\n",
      "published:\n"
     ]
    }
   ],
   "source": [
    "summarize(lg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = load_summarize_chain(llm,chain_type='refine',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"METHODS\n",
      "published: 23 August 2021\n",
      "doi: 10.3389/fnana.2021.712842\n",
      "Edited by:\n",
      "Zoltan F. Kisvarday,\n",
      "University of Debrecen, Hungary\n",
      "Reviewed by:\n",
      "Tetsufumi Ito,\n",
      "University of Toyama, Japan\n",
      "Marcel Oberlaender,\n",
      "Center of Advanced European\n",
      "Studies and Research (CAESAR),\n",
      "Germany\n",
      "*Correspondence:\n",
      "Tingwei Quan\n",
      "quantingwei@hust.edu.cn\n",
      "Received: 21 May 2021\n",
      "Accepted: 29 July 2021\n",
      "Published: 23 August 2021\n",
      "Citation:\n",
      "Huang Q, Cao T, Chen Y, Li A,\n",
      "Zeng S and Quan T\n",
      "(2021) Automated Neuron Tracing\n",
      "Using Content-Aware Adaptive Voxel\n",
      "Scooping on CNN Predicted\n",
      "Probability Map.\n",
      "Front. Neuroanat. 15:712842.\n",
      "doi: 10.3389/fnana.2021.712842\n",
      "Automated Neuron Tracing Using\n",
      "Content-Aware Adaptive Voxel\n",
      "Scooping on CNN Predicted\n",
      "Probability Map\n",
      "Qing Huang 1,2, Tingting Cao 1,2, Yijun Chen 1,2, Anan Li 1,2, Shaoqun Zeng 1,2\n",
      "and Tingwei Quan 1,2*\n",
      "1Britton Chance Center for Biomedical Photonics, Wuhan National Laboratory for Optoelectronics-Huazhong University of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point:  A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. This study is conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Science and Technology, Wuhan, China, 2MoE Key Laboratory for Biomedical Photonics, Collaborative Innovation Center for\n",
      "Biomedical Engineering, School of Engineering Sciences, Huazhong University of Science and Technology, Wuhan, China\n",
      "Neuron tracing, as the essential step for neural circuit building and brain information\n",
      "ﬂow analyzing, plays an important role in the understanding of brain organization and\n",
      "function. Though lots of methods have been proposed, automatic and accurate neuron\n",
      "tracing from optical images remains challenging. Current methods often had trouble in\n",
      "tracing the complex tree-like distorted structures and broken parts of neurite from a\n",
      "noisy background. To address these issues, we propose a method for accurate neuron\n",
      "tracing using content-aware adaptive voxel scooping on a convolutional neural network\n",
      "(CNN) predicted probability map. First, a 3D residual CNN was applied as preprocessing\n",
      "to predict the object probability and suppress high noise. Then, instead of tracing on\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. This study, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, aims to address the challenges of tracing complex tree-like distorted structures and broken parts of neurites from a noisy background. To do this, a 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "the binary image produced by maximum classiﬁcation, an adaptive voxel scooping\n",
      "method was presented for successive neurite tracing on the probability map, based on\n",
      "the internal content properties (distance, connectivity, and probability continuity along\n",
      "direction) of the neurite. Last, the neuron tree graph was built using the length ﬁrst\n",
      "criterion. The proposed method was evaluated on the public BigNeuron datasets and\n",
      "ﬂuorescence micro-optical sectioning tomography (fMOST) datasets and outperformed\n",
      "current state-of-art methods on images with neurites that had broken parts and complex\n",
      "structures. The high accuracy tracing proved the potential of the proposed method for\n",
      "neuron tracing on large-scale.\n",
      "Keywords: neuronal image, tubular object tracing, content-aware adaptive voxel tracing, 3D CNN, high precision\n",
      "INTRODUCTION\n",
      "Digital reconstruction or tracing of neurons, which converts a neuronal image into a digital\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. This study, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, aims to address the challenges of tracing complex tree-like distorted structures and broken parts of neurites from a noisy background. To do this, a 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping and a binary image produced by maximum classification. The proposed method was evaluated on the public BigNeuron datasets and fluorescence micro-optical sectioning tomography (fMOST) datasets and outperformed current state-of-art methods on images with neurites that had broken parts and complex structures. The high\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "representation by obtaining the 3D spatial position of neuron skeletons and building their\n",
      "topological connections, is one of the major subjects in computational neuroscience (Parekh and\n",
      "Ascoli, 2013). Neuron tracing is regarded as the basis of the neuronal morphological study, neuron\n",
      "phenotype identification, and neural circuit building, which is important in understanding the brain\n",
      "organization and function in diseases like Alzheimer and Schizophrenia (Economo et al., 2016; Lin\n",
      "et al., 2018). However, current neuron tracing is mainly performed by hand, it could take hours of\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "1\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. This study, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, aims to address the challenges of tracing complex tree-like distorted structures and broken parts of neurites from a noisy background. To do this, a 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping and a binary image produced by maximum classification. The proposed method was evaluated on the public BigNeuron datasets and fluorescence micro-optical sectioning tomography (fMOST) datasets and outperformed current state-of-art methods on images with neurites that had broken parts and complex structures. The high\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Huang et al.\n",
      "Content-Aware Adaptive Voxel Scooping Tracing\n",
      "hard work to trace a simple dendritic tree and months of labor for\n",
      "large-scale neurons (Economo et al., 2016). The lack of automatic\n",
      "and accurate tracing method has become a critical technical\n",
      "bottleneck in neuroscience research.\n",
      "Neuron\n",
      "tracing\n",
      "from\n",
      "optical\n",
      "microscopy\n",
      "images\n",
      "was\n",
      "challenging.\n",
      "First,\n",
      "the\n",
      "high\n",
      "artifacts\n",
      "and\n",
      "noises\n",
      "in\n",
      "the\n",
      "neuronal image fuzzy the neurites (see Figure 1), and easily\n",
      "lead to inaccurate tracing. Second, the neuron structure\n",
      "is complex and distorted with various direction changes,\n",
      "even an experienced annotator had to spend hours to trace\n",
      "these tree-like or mushroom-like structures from the image\n",
      "(Figures 1B1–E1). Third, the intensity distribution of neurites\n",
      "is often inhomogeneous and plenty of broken neurite parts exist\n",
      "due to the sudden intensity changes along the neurites, which\n",
      "leads to an interruption of neuron tracing (as pointed out by\n",
      "arrows in Figure 1). These are also common problems of tubular\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures and broken parts of neurites from a noisy background. To do this, a 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping and a binary image produced by maximum classification. This method was evaluated on the public BigNeuron and fluorescence micro-optical sectioning tomography (fMOST) datasets and outperformed state-of-art methods on images with neurites that had broken parts, complex structures, high artifacts and noises,\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "objects tracing of medical images, including retinal, liver vessel,\n",
      "and brain vessel tracking.\n",
      "Currently,\n",
      "numerous\n",
      "semi-automatic\n",
      "or\n",
      "automatic\n",
      "algorithms have been proposed for neuron tracing. APP2 used\n",
      "the image intensity and space information for gray-weighted\n",
      "image distance-tree calculation and applied hierarchical pruning\n",
      "for refinement (Xiao and Peng, 2013). Open-Snake traced the\n",
      "neurites by adapting open-curve snake based on the combination\n",
      "of a gradient vector flow, neurite orientations, and a set of control\n",
      "rules (Wang et al., 2011). neuTube applied a model-based\n",
      "algorithm for neuron tracing using the local geometrical and\n",
      "global structures of neurite (Zhao et al., 2011; Feng et al., 2015).\n",
      "Tubularity-Flow-Field realized neuron tracing via centerline\n",
      "extraction on segmentation achieved by applying directional\n",
      "regional growing based on the direction of neurite tubularity\n",
      "(Mukherjee et al., 2015). RPCT, NeuroGPS, and SparseTracer\n",
      "used principal curves tracing, and some used direction constrains\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point:  \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures and broken parts of neurites from a noisy background. To do this, a 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping and a binary image produced by maximum classification. This method was evaluated on the public BigNeuron and fluorescence micro-optical sectioning tomography (fMOST) datasets and outperformed state-of-art methods on images with neurites that had broken parts, complex structures, high artifacts and noises\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "for reconstruction (Bas and Erdogmus, 2011; Quan et al., 2016;\n",
      "Li et al., 2017b). FMST adopted fast marching and minimum\n",
      "spanning tree for faster tracing without loss of small branches\n",
      "(Yang et al., 2019). Some smart machine-learning-based methods\n",
      "have also been applied for neuron tracing. SmartTracing (Chen\n",
      "et al., 2015) and ST-LVF (Li et al., 2019b) first extracted different\n",
      "local features based on the tracing results of existed algorithms\n",
      "(APP2 and SparseTracer respectively), then applied support\n",
      "vector machine (SVM) to promote complete neuron tracing\n",
      "from noisy image. These algorithms combined the advantages\n",
      "of various image processing methods by considering different\n",
      "neurite characters for neuron tracing. They were usually sensitive\n",
      "to parameters setting and hard to trace these complex-structure\n",
      "and broken neurons. Besides, the self-learning smart algorithms\n",
      "relied on previous tracing and were computationally complicated\n",
      "and time-consuming (Chen et al., 2015; Li et al., 2019b).\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures and broken parts of neurites from a noisy background. To do this, a 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping and a binary image produced by maximum classification. This method was evaluated on the public BigNeuron and fluorescence micro-optical sectioning tomography (fMOST) datasets and outperformed state-of-art methods, such as Bas and Erdogmus, 2011; Quan et al., 2016; Li\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Deep convolutional neural network (CNN) automatically\n",
      "extracted more discriminative image features and outperformed\n",
      "traditional algorithms in image segmentation (Çiçek et al., 2016;\n",
      "Li et al., 2017a; Chen et al., 2018). The technique was also\n",
      "employed in neuron tracing. DeepNeuron applied 2D CNN for\n",
      "neurite detection and connection, and showed low performance\n",
      "on neurite tracing from 3D noisy image (Zhou et al., 2018). Li\n",
      "et al. (2017a) applied 3D CNN to suppress the high noise to\n",
      "improve the tracing performance of the previous tracing method\n",
      "of APP2. Recently, weakly-supervised learning was developed\n",
      "to allow automatic training labels’ generation to promote deep\n",
      "learning usage in neuron tracing (Huang et al., 2020). From these\n",
      "previous works, we found they still had difficulties in tracing\n",
      "these neurites with distorted and branches structures or broken\n",
      "parts. These methods generally use the strong prediction of CNN,\n",
      "such as the prediction by maximum classification, and some\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point:  \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures and broken parts of neurites from a noisy background. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. A 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping and a binary image produced by maximum classification. This method was evaluated on the public BigNeuron and fluorescence micro-optical sectioning tomography (fMOST\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "limited setting rules for neuron tracing and graph tree building.\n",
      "Thus, their performance decreased when tracing these above\n",
      "hard neurites.\n",
      "In the article, we proposed an automatic and accurate\n",
      "method for neuron tracing to address the difficulties of neurons\n",
      "with complex structures and uneven intensity distribution. We\n",
      "estimated the neurite probability map from a noisy image\n",
      "by 3D residual CNN. Unlike previous methods, we took full\n",
      "utilization of the probability characters and proposed a content-\n",
      "aware adaptive voxel scooping method for continuous tracing of\n",
      "broken neurites, which is based on the distance, connectivity, and\n",
      "directional probability continuity. We evaluated the proposed\n",
      "method on the public BigNeuron datasets (Peng et al., 2015)\n",
      "from different organizations and fluorescence micro-optical\n",
      "sectioning tomography (fMOST) datasets (Gong et al., 2013).\n",
      "Our method achieved comparable results of manual tracing and\n",
      "was superior to some current algorithms on neuronal images\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. A 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping and a binary image produced by maximum classification. This method was evaluated on the public BigNeuron and fluorescence micro-optical sectioning tomography (\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "with complicatedstructure and broken neurites.\n",
      "MATERIALS AND METHODS\n",
      "The flowchart of the proposed neuron tracing method is\n",
      "shown in Figure 2. It includes two steps: (1) Preprocessing:\n",
      "predicting neurite probability map by 3D deep residual CNN.\n",
      "(2) Continuous neurite tracing of broken parts on the probability\n",
      "map by voxel-scooping (VS) based content-aware adaptive\n",
      "tracing (CAAT).\n",
      "Preprocessing: Probability Prediction by\n",
      "3D Residual CNN\n",
      "In preprocessing, the neurite probability is estimated by 3D\n",
      "CNN, considering its ability in automatic feature extraction, to\n",
      "detect the fuzzy neurite from a noisy image. Instead of using\n",
      "the very complicated network module (more than 200 layers)\n",
      "in Li et al. (2017a), the 3D deep voxelwise residual network\n",
      "(VoxResNet, 25 layers; Chen et al., 2018) is employed as our\n",
      "segmentation model for its simple and elegant structure, deep\n",
      "supervised mechanism, and outstanding predictive ability. The\n",
      "detailed network architecture is referred to in Chen et al. (2018).\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. A 3D residual CNN was applied as preprocessing to predict the object probability and suppress high noise, followed by tracing on the CNN probability map using content-aware adaptive voxel scooping and a binary image produced by maximum classification. This method was evaluated on the public BigNeuron and fluorescence micro-optical sectioning tomography\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "In the training, randomly cropped image samples with\n",
      "overlapped regions and size 120 × 120 × 120 or 64 × 64 × 64 are\n",
      "used for training considering the computational power and\n",
      "accuracy. To avoid serious imbalanced classes between the\n",
      "foreground (neurite) and background, samples with few\n",
      "foreground voxels are abandoned. To increase the diversity of\n",
      "samples and improve network robustness, we apply random\n",
      "rotation, flip, contrast, brightness adjustment, and Gaussian blur\n",
      "for data augmentation.\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "2\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur to increase diversity and improve robustness. This was followed by tracing on the CNN probability map using content\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Huang et al.\n",
      "Content-Aware Adaptive Voxel Scooping Tracing\n",
      "FIGURE 1 | Examples of 3D optical neuronal images with complex tree-like distorted structures and broken parts of the neurite. (A1,B1) Images from ﬂuorescence\n",
      "micro-optical sectioning tomography (fMOST) datasets. (C1–E1) Images from BigNeuron datasets. (A2–E2) Corresponding manual tracing results of (A1–E1). Yellow\n",
      "circles point out some twisted structures with various directions. Red arrows point to some broken neurite parts with the sudden change of intensity.\n",
      "FIGURE 2 | Flowchart of the proposed neuron tracing method.\n",
      "To further prevent the prediction bias of imbalanced classes\n",
      "and improve accuracy, we combined the dice loss and weighted\n",
      "cross-entropy loss as the hybrid loss (Huang et al., 2020), which\n",
      "is defined as follows:\n",
      "losshybird = (1 − 2\n",
      "�m\n",
      "i = 1 pigi + ε\n",
      "�m\n",
      "i = 1 pi + �m\n",
      "i = 1 gi + 1)\n",
      "+ σ\n",
      "� �m\n",
      "i = 1 −\n",
      "�m\n",
      "i = 1 gi\n",
      "m\n",
      "· gi log(pi)\n",
      "�\n",
      "(1)\n",
      "Where Pi is the predicted probability of pixel i. gi is the label value\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur to increase diversity and improve robustness. To further prevent the prediction bias of imbalanced classes and improve\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "with 1 for foreground and 0 for background. σ is the weighted\n",
      "parameter and set to 0.5 in the experiment.\n",
      "Neuron Tracing\n",
      "With\n",
      "proper\n",
      "training,\n",
      "CNN\n",
      "could\n",
      "achieve\n",
      "expert-level\n",
      "performance on neurite prediction and even had low predictions\n",
      "for the fuzzy and broken neurites in a noisy background. Here,\n",
      "we take full usage of the CNN predicted probability using object’s\n",
      "internal properties and probability continuity, and propose an\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "3\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, explores automated neuron tracing using content-aware adaptive voxel scooping on CNN predicted probability maps. Conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, the study aimed to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur to increase diversity and improve robustness. To further prevent the prediction bias of imbalanced classes and improve\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Huang et al.\n",
      "Content-Aware Adaptive Voxel Scooping Tracing\n",
      "automatic neuron tracing method. The method includes four\n",
      "steps: (1) Initial segmentation of probability map; (2) Seed\n",
      "point selection; (3) Neurite tracing using VS based CAAT; and\n",
      "(4) Neuron tree graph establishing and branch pruning for\n",
      "simplicity.\n",
      "The initial neurite segmentation should remain good neurite\n",
      "shape meanwhile contain more neurite parts. An initial threshold\n",
      "estimation method based on the intensity distribution and\n",
      "heuristic rules of the probability is applied for better performance\n",
      "than maximum classification.\n",
      "First, we estimate the adaptive threshold of the neurite\n",
      "probability predicted by CNN, instead of using the typical\n",
      "threshold 0.5 to classify the foreground from the background of\n",
      "the prediction. A Gaussian function is used to fit the background\n",
      "probability histogram of the prediction, the histogram with\n",
      "probability <0.5, to estimate the mean µbg and standard\n",
      "deviation σ bg of the background probability. Then, the adaptive\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study aims to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. To further prevent the prediction bias of imbalanced\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "threshold is set to µbg+λσ bg. Where λ is set to 3 to exclude more\n",
      "background noise meanwhile keep most foreground signals.\n",
      "The first points of every connected binary region are selected\n",
      "as seed points for the subsequent automatic tracing. In tracing,\n",
      "the skeleton points (or tracing nodes) of the neurites and their\n",
      "connection relationships are built. We trace the neurites from\n",
      "the predicted probability using VS based CAAT. The VS tracing\n",
      "approach starts from the seed points inside the object, and\n",
      "iteratively generates new voxel clusters around the previous\n",
      "cluster along the object based on the region connectively and\n",
      "spatial position. The searched clusters are then used to produce\n",
      "the skeleton node of the object and build the spatial connection\n",
      "of the nodes. For precise and fast tracing, we apply VS for\n",
      "normal tracing of the connected neurite regions on initial\n",
      "segmentation Iseg, and CAAT for continuous tracing of broken\n",
      "neurite fragments based on the probability image Pimg.\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study aims to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. To further prevent the prediction bias of imbalanced\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Here, we define the current point set as Cps, and the next\n",
      "point set as Nps. The center point of Cps and Nps are defined\n",
      "as the current tracing node TNk and the next tracing node\n",
      "TNk+1, respectively. For normal tracing, Nps are searched by\n",
      "VS using Cps as seed point set based on 26-connected domain\n",
      "and scooping distance. The maximum distance of TNk and\n",
      "unvisited object voxels in the 26-connected neighborhood of Cps\n",
      "is calculated as the scooping distance. Any of the unvisited object\n",
      "voxel that falls into the ball of the scooping distance of TNk is\n",
      "searched and added into Nps for accurate estimation in case of\n",
      "directional changes. For discontinuous tracing, if Nps couldn’t\n",
      "be searched by VS, the new point sets Sps is then searched by\n",
      "CAAT and used as Nps [detailed process of CAAT is in Section\n",
      "‘‘Content-Aware Adaptive Tracing (CAAT)’’]. The searched Nps\n",
      "is used to update Cps iteratively, and the tracing continues until\n",
      "no new points are searched.\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. The study aims to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. To further prevent the prediction bias of imbalanced\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "After tracing, the tree graph of the neurites is built based on\n",
      "the neighborhood connection relationship and spatial position\n",
      "of the traced skeleton nodes. Three kinds of nodes are defined to\n",
      "build the tree graph: leaf node, path node, and branch node. Leaf\n",
      "node exists at the end of a neurite that with only one immediate\n",
      "neighboring node. Path node exists between the leaf node and\n",
      "branch node along the neurite path with only two immediate\n",
      "neighbors. Branch node is the intersect point of crossed branches\n",
      "with more than two immediate neighbors (Li et al., 2019a). In the\n",
      "graph tree building, we apply the length first criterion. The nearly\n",
      "longest neurite path in each individual tree is first chosen out as\n",
      "the stem, which starts from the seed point and ends at the last\n",
      "node with nearby nodes linked by line based on their connection\n",
      "relationship. Other neurite paths that form the branch nodes of\n",
      "the truck to the leaf nodes are defined as the tree branch. To\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. The study aims to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. After tracing, the tree graph of the neur\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "remove some spurious end nodes that are caused by neuronal\n",
      "irregularities such as the expansion of neurite part or bouton,\n",
      "we prune short branches. If the skeleton point number from a\n",
      "leaf node to a branch node of a neurite path is less than lnum, the\n",
      "branch is pruned. lnum can be set between 5 and 10, and is set to\n",
      "6 in the experiment. Finally, the established neuron tree graph\n",
      "is saved in SWC file format that records the nodes’ sequence,\n",
      "position, and connection, and can be accessed by the commercial\n",
      "software Amira and Neurolucida.\n",
      "Content-Aware Adaptive Tracing (CAAT)\n",
      "For expert observers, the distance, region connection attributes,\n",
      "and probability properties between neurite fragments are\n",
      "combined to determine their connection relationship. In the\n",
      "article, we simulate the judging process of the experts and\n",
      "propose the CAAT method by using three content perception\n",
      "terms\n",
      "(distance,\n",
      "connectivity,\n",
      "and\n",
      "direction\n",
      "probability\n",
      "continuity term) for connection. Detailed processing steps\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. The study aims to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. After tracing, the tree graph of the neur\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "are described in Algorithm 1.\n",
      "Distance Term\n",
      "The distance between nearby broken parts of an object should\n",
      "be in a certain range. A closer distance between current points\n",
      "Cps and searched points Sps, a higher connection probability. The\n",
      "term is given by:\n",
      "dscore =\n",
      "�\n",
      "1\n",
      "; d ≤ dt\n",
      "exp(− d−dt\n",
      "3 )\n",
      "; d > dt\n",
      "(2)\n",
      "Here, we find the closest point pairs (cp, sp) of Cps and Sps by\n",
      "traversal search. d =\n",
      "����cp − sp\n",
      "����\n",
      "∞ is the Chebyshev distance\n",
      "between the point pairs. dt is the distance parameter and also\n",
      "the only parameter of CAAT. It is set to 4–5 pixels based on\n",
      "experience to prevent over-connection of nearby neurites or\n",
      "under-connection of discontinuous neurites.\n",
      "Connectivity Term\n",
      "To prevent repeated tracing caused by the trace of unvisited\n",
      "points that belong to the same connected domain of Cps (like\n",
      "neurite irregularities), Cps and Sps must belong to different\n",
      "connected domains (region 1 and 2 in Figure 2 S2), and Cps ⊆\n",
      "Ri, Sps ⊆ Rj. The term is defined:\n",
      "cscore =\n",
      "� 0;\n",
      "Ri = Rj\n",
      "1;\n",
      "Ri ̸= Rj\n",
      "(3)\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. The study aims to address the challenges of tracing complex tree-like distorted structures, broken parts of neurites, and uneven intensity distributions from a noisy background. After tracing, the tree graph of the neur\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Where Ri and Rj are the 2- connected domains that contain Cps\n",
      "and Sps, respectively.\n",
      "Direction Probability Continuity Term\n",
      "Various image content and appearance features are combined\n",
      "into the CNN estimated neurite probability. For broken ones,\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "4\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. Various image content and appearance features are combined into the CNN estimated neurite probability, which is then used to calculate the Direction Probability Continuity Term between two connected domains (Ri and Rj)\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Huang et al.\n",
      "Content-Aware Adaptive Voxel Scooping Tracing\n",
      "the probabilities along the direction of −→\n",
      "cpsp are higher, the\n",
      "connection probability is larger. The term is calculated:\n",
      "dpcscore = exp(−(dps −\n",
      "�\n",
      "CP(Mps)/dps))\n",
      "(4)\n",
      "Where CP is a continuity probability function that modifies the\n",
      "neurite probability. Mps are connected points along the line of\n",
      "−→\n",
      "cpsp. dps is the voxel number of Mps.\n",
      "CP(i) =\n",
      "� 1,\n",
      "Pimg(i) > tl\n",
      "Pimg(i),\n",
      "Pimg(i) ≤ tl\n",
      "(5)\n",
      "Here, tl is a low threshold used to discover more possible neurite\n",
      "voxels between cp and sp, and excludes at least 50% background\n",
      "noise. It is estimated based on the background probability\n",
      "histogram.\n",
      "min\n",
      "� �\n",
      "k\n",
      "����Pimg(k) ≤ tl\n",
      "����\n",
      "0 ≥ 0.5\n",
      "�\n",
      "k\n",
      "����Pimg(k) ≤ 0.5\n",
      "����\n",
      "0\n",
      "�\n",
      "(6)\n",
      "And refined as tl =min(0.1, tl). Finally, the adaptive linking score\n",
      "is built based on the three above content terms.\n",
      "linkscore = dscore · cscore · dpcscore\n",
      "(7)\n",
      "When linkscore > 0.5, Sps is used as Nps for continuous tracing of\n",
      "broken neurites.\n",
      "Evaluation Metrics\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. Various image content and appearance features are combined into the CNN estimated neurite probability, which is then used to calculate the Direction Probability Continuity Term between two connected domains (Ri and Rj).\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "The typical metrics of precision and recall are used for objective\n",
      "evaluation of the proposed and current novel neuron tracing\n",
      "methods (Quan et al., 2016; Li et al., 2017b). We first equally\n",
      "resampled the skeleton points to keep the same distance of\n",
      "adjacent points (1 pixel). Here, the manual tracing results are\n",
      "used as the gold standard. If the closest distance of a traced\n",
      "skeleton point by the algorithm to the gold standard is less than\n",
      "6 pixels, the point is defined as a true positive (TP) point (Li\n",
      "et al., 2017b). The parameter was set to 6 in our application,\n",
      "as we resampled the skeleton points to the same size and the\n",
      "experiment was mainly performed on neuronal images that\n",
      "contained axon neurites or dendrite and axon with similar radius\n",
      "(generally 4–8 pixels). The precision and recall rates kept stable\n",
      "when the parameter ranged from 6 to 10 pixels (1 µm/pixel;\n",
      "Quan et al., 2016), and 6 was reasonable in our experiment.\n",
      "Precision and recall are used to reflect tracing accuracy and\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. Various image content and appearance features are combined into the CNN estimated neurite probability, which is then used to calculate the Direction Probability Continuity Term between two connected domains (Ri and Rj).\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "integrity and calculated as the number of TP points to the\n",
      "number of skeleton points obtained from algorithms and manual\n",
      "tracing, respectively. FN means false negative skeleton points.\n",
      "precision = TP/(TP + FP)\n",
      "recall = TP/(TP + FN)\n",
      "(8)\n",
      "The morphological parameters including the neuron path\n",
      "lengths and branch points are also calculated in the study for\n",
      "evaluation of neuron morphology.\n",
      "Experimental Setup\n",
      "The public BigNeuron datasets (Peng et al., 2015) and private\n",
      "fMOST datasets (Gong et al., 2013) were used for evaluation.\n",
      "This study focuses on precise tracing on discontinuous neurites\n",
      "(mainly axons, as the dendrites were generally easy to identify\n",
      "with brighter and thicker neurites than axons). Therefore, we\n",
      "selected 38 neuronal images from BigNeuron dataset that had\n",
      "the same bit depth (0–255), available standard reconstruction,\n",
      "and contained noises, rich axons with thin, dim, and broken\n",
      "neurites as our training (30 subjects) and testing datasets\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. Randomly cropped image samples with overlapped regions and size 120×120×120 or 64×64×64 were used for training, with data augmentation techniques such as random rotation, flip, contrast, brightness adjustment, and Gaussian blur. Various image content and appearance features are combined into the CNN estimated neurite probability, which is then used to calculate the Direction Probability Continuity Term between two connected domains (Ri and Rj).\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "(eight subjects). These neuron images were from different\n",
      "organizations or projects including the Utokyo, Janeliar Fly\n",
      "Light, Allen Institute, and Taiwan FlyCircuits. They contained\n",
      "various species including cells of fly, mice, and humans. They\n",
      "were generated using different imaging methods including\n",
      "confocal and 2-photon. Their image resolutions were between\n",
      "0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image\n",
      "sizes were between 511–2,048 × 511–2,048 × 11–537. The\n",
      "fMOST image was generated using micro-optical sectioning\n",
      "tomography imaging (Li et al., 2010) on a C57BL/6J mouse\n",
      "brain labeled by AAV virus in the cortex. The image resolution\n",
      "was 0.2 µm × 0.2 µm × 1 µm. We selected 48 image blocks\n",
      "that contained abundant and discontinuous axon neurites from\n",
      "the fMOST dataset as training set. Their image sizes were\n",
      "300 × 300 × 300. We tested the algorithm on 16 image blocks\n",
      "with sizes between 164–1,000 × 214–1,000 × 300. These training\n",
      "and testing image blocks were captured from different brain\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employs deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. The study used 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits. The images were generated using different imaging methods such as confocal and 2-photon, and had image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "areas. We also evaluated the proposed method on a large-scale\n",
      "neuronal image with image size of 3,630 × 3,630 × 1,080. In the\n",
      "experiment, the 3D CNN was implemented on Pytorch, and the\n",
      "tracing method was implemented on Matlab. The CPU was Intel\n",
      "i7-6850K (64 GB RAM) and GPU was NVIDIA 1080Ti.\n",
      "RESULTS\n",
      "In the proposed method, neurite probability is estimated\n",
      "by 3D residual CNN as preprocessing. Figure 3 shows the\n",
      "prediction performance on neuronal images that have high\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "5\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. The study used 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Huang et al.\n",
      "Content-Aware Adaptive Voxel Scooping Tracing\n",
      "noises, complicated twisted structures and broken neurite\n",
      "fragments. As we can see, the trained CNN effectively reduced\n",
      "the high artifacts and noises of the optical images. These neurons\n",
      "with unevenly distributed intensities along the neurites and\n",
      "various twisted structures and branches were generally predicted\n",
      "by the CNN accurately (Figures 3F–J). Only for these hard parts\n",
      "that had broken fragments or inhomogeneous areas that with\n",
      "sudden intensity and direction changes, the predictions seemed\n",
      "to be discontinuous and there still had some low predictions for\n",
      "these parts, which was quite similar to the estimated probability\n",
      "of humans for uncertain objects. The comparison in Figures 5–8\n",
      "further illustrated the network effectiveness on these images. The\n",
      "results proved that the presented network was precise and robust\n",
      "in neurite prediction from noisy images.\n",
      "To illustrate the effect of the proposed CAAT in neuron\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. The study used 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "tracing of broken parts and complex structures, we compared\n",
      "the performance of CAAT and VS using the typical threshold-\n",
      "based probability classification method (see Figure 4). The\n",
      "probabilities for broken neurite parts were discontinuous.\n",
      "A higher classification threshold would lead to a severe\n",
      "fracture\n",
      "phenomenon.\n",
      "A\n",
      "lower\n",
      "classification\n",
      "threshold\n",
      "would make the tracing result be interfered with by the\n",
      "background noise and deviated from normal tracing at\n",
      "complex neurite structures, such as bifurcation, distortion,\n",
      "and turning. A proper threshold for probability classification\n",
      "was hard to be found due to the abundant appearances of\n",
      "an image with uneven intensities. Using adaptive tracing\n",
      "strategy, the performance of CAAT was nearly the same\n",
      "as\n",
      "the\n",
      "manual\n",
      "tracings\n",
      "and\n",
      "outperformed\n",
      "VS\n",
      "tracing\n",
      "using\n",
      "threshold-based\n",
      "classification\n",
      "method\n",
      "on\n",
      "predicted\n",
      "probability map.\n",
      "We further performed an ablation study to illustrate the\n",
      "effectiveness of CNN prediction and CAAT of the proposed\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point:  \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation. The study used 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "method. For validation of CNN prediction effectiveness, current\n",
      "typical and novel enhancement methods [Jerman (Jerman et al.,\n",
      "2015) and CAEFI (Jeelani et al., 2019)] were applied to replace\n",
      "CNN prediction for the next tracing by CAAT. Jerman filter\n",
      "was an improved multiscale vesselness filter for objects with\n",
      "different radii and could be used for neurite enhancement\n",
      "considering their tubular structures (Wang et al., 2011; Basu\n",
      "et al., 2013; Zhou et al., 2015). CAEFI (Jeelani et al., 2019) was\n",
      "a novel enhancement method for image containing filamentous\n",
      "structures, by combining the characteristics of gradient sparsity\n",
      "and filamentous structure constraint for effective removal of\n",
      "high noises. The comparisons were performed on both the\n",
      "BigNeuron and fMOST images that contained high noises,\n",
      "complex neuron structures, and discontinuous neurites with\n",
      "different radius (see Figures 5, 6B). For good enhancement\n",
      "performances, the parameters of Jerman and CAEFI were\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation, using 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. For\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "manually adjusted. The scale parameters of Jerman were set to\n",
      "1–3 or 1–4, and the cutoff threshold was set to 0.75 or 0.6,\n",
      "respectively. The window size of CAEFI was set to 9, and the\n",
      "smooth parameter was set to 0.002 and 0.0001 for BigNeuron\n",
      "and fMOST images, respectively. As seen in Figures 5, 6B,C,\n",
      "the Jerman, CAEFI and CNN prediction all suppressed high\n",
      "noises of neuronal images. Jerman filter enhanced neurites\n",
      "with different radii. The filter would thicken the neurites and\n",
      "overlapped nearby neurites, and led to the missing tracing\n",
      "of close neurites (Figure 5B). Jerman filter also showed low\n",
      "performances on discontinuous neurites for the weak estimation\n",
      "at uneven objects, and large fragments existed (Figure 6B).\n",
      "CAEFI effectively removed cluster and noise and obtained a\n",
      "clear background. The boundary of the neuron showed hairy\n",
      "structures and led to over-tracing of close neurites (Figure 5B).\n",
      "The gradient smooth constraint strategy wiped up the thin\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation, using 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. For\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "or unobvious objects due to their indistinctive gradients and\n",
      "caused the discontinuous tracing of uneven neurites (Figure 6B).\n",
      "CNN showed a clear prediction of complex structures and\n",
      "discontinuous neurites, promoted proper neurite tracing by\n",
      "CAAT method, and outperformed the Jerman and CAEFI\n",
      "enhancement methods in these cases.\n",
      "For validation of CAAT effectiveness, automatic and state-of-\n",
      "art tracing methods [FMST (Yang et al., 2019) and OpenSnake\n",
      "(Wang et al., 2011)] were applied to replace CAAT after CNN\n",
      "prediction. To deal with discontinuous neurites, FMST first\n",
      "adapted the over-reconstruction strategy of neurons, and merged\n",
      "broken neurites after the pruning step (Yang et al., 2019),\n",
      "OpenSnake realized automatic snake merging after branching\n",
      "point detection (Wang et al., 2011). These methods proved good\n",
      "performance on discontinuous neurite tracing, while over-traced\n",
      "or over-connected to some nearby noises or neurites in some\n",
      "cases (Figures 5, 6B). CAAT made a more proper judgment\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features and outperform traditional algorithms in image segmentation, using 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. For\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "than the two tracing methods to decide whether two broken\n",
      "fragments should connect or not, and achieved fine tracing of\n",
      "the neuron with no connection to nearby neurites. As marked\n",
      "out in Figure 5, CAAT could more accurately trace close\n",
      "neurites than FMST, and get the correct connection relationship\n",
      "of nearby or branching neurites. These performances proved\n",
      "that the CNN prediction and CAAT both achieved similar\n",
      "or better performances than current novel enhancement or\n",
      "tracing methods, respectively. The proposed method combined\n",
      "the advantages of CNN prediction and CAAT, and obtained\n",
      "identical tracings of manual tracing results, especially for neurons\n",
      "with complex structures or uneven intensity distribution along\n",
      "neurites.\n",
      "We compared the performances of the proposed method\n",
      "and current novel algorithms on the public BigNeuron\n",
      "datasets, including the typical APP2 (Xiao and Peng, 2013),\n",
      "neuTube (Feng et al., 2015), machine-learning based ST-LVF\n",
      "(Li et al., 2019b), and CNN+APP2 algorithm similar to Li\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, using 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of the\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "et al. (2017a), which was realized by applying APP2 on the\n",
      "adjusted neuronal image using CNN predictions. As shown\n",
      "in Figure 7, the neuronal morphologies were complicated in\n",
      "BigNeuron datasets, which had mushroom-like structures, lots\n",
      "of twisted neurites, and uneven intensity distributions. On high\n",
      "noisy images, APP2 and ST-LVF algorithms, which achieved\n",
      "the initial segmentation by using threshold-based methods,\n",
      "misidentified the background noise as the signals and lead to\n",
      "large tracing errors of the noises. On images with complex\n",
      "neuronal structures, APP2 was hard to reconstruct the precise\n",
      "morphologies, it missed a lot of neurite branches and went\n",
      "in the wrong direction of the twisted structures. neuTube\n",
      "missed some neurite branches and wrongly connected some\n",
      "separated nearby neurites. ST-LVF tended to over-reconstruct\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "6\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, using 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of the\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Huang et al.\n",
      "Content-Aware Adaptive Voxel Scooping Tracing\n",
      "FIGURE 3 | Examples of 3D CNN prediction (F–J) on noisy optical images (A–E) with complex-structures and broken neurites. Red arrows point to the broken\n",
      "neurite parts on the original image and corresponding probability.\n",
      "FIGURE 4 | Comparison of the proposed content-aware adaptive tracing (CAAT) (H) and typical threshold-based classiﬁcation method (B–D,F,G) on neuron tracing\n",
      "with the same initial seed point. Optical image (A); gold standard (E). Inaccurate traced areas were ampliﬁed in the white box.\n",
      "the twisted structures and generated lots of wrong branches.\n",
      "CNN+APP2 also missed lots of twisted neurites as a result of\n",
      "APP2 algorithm. On images with uneven neurites and broken\n",
      "parts, APP2, neuTube, and CNN+APP2 algorithms failed in the\n",
      "tracing of some broken parts which caused large incomplete\n",
      "neurite tracings. The proposed method can trace the precise\n",
      "neuron morphology even in noisy images with discontinuous\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, using 8 subjects with neuron images from different organizations or projects including the Utokyo, Janeliar Fly Light, Allen Institute, and Taiwan FlyCircuits, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm, and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of the\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "neurites and complex neuronal structures. As proved in Table 1,\n",
      "the average precision of our method was 98.2%, which was\n",
      "similar to the results of CNN+APP2, and better than that of\n",
      "APP2, neuTube, and ST-LVF. The average recall of our method\n",
      "was 95.1%, which was vastly better than that of APP2, neuTube,\n",
      "and CNN+APP2. The compared average high precision and\n",
      "recall values proved that our method can more effectively reduce\n",
      "high noise interferences and achieve more complete and accurate\n",
      "tracings of complex neurons than current methods.\n",
      "We also compared the performances of our method and\n",
      "the above methods (Xiao and Peng, 2013; Feng et al., 2015; Li\n",
      "et al., 2017a, 2019b) on fMOST datasets. As shown in Figure 8,\n",
      "the datasets contained a huge amount of unevenly distributed\n",
      "signals with broken parts and high noises. APP2 showed\n",
      "poor performance on these datasets, either over-reconstructed\n",
      "large background noises or hardly traced the broken neurites.\n",
      "neuTube and ST-LVF algorithms wrongly reconstructed some\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of the study proved that the CAAT on CNN predicted probability maps can more effectively reduce high noise interferences and achieve more complete and accurate tracings of complex neurons than current\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "background noises, and led to a number of neurite fragments.\n",
      "The algorithm of CNN+APP2 can prevent the noise interference,\n",
      "while the performances for broken neurites were not good. The\n",
      "proposed method accurately traced these neurites and avoided\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "7\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of the study proved that the CAAT on CNN predicted probability maps can more effectively reduce high noise interferences and achieve more complete and accurate tracings of complex neurons than current\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Huang et al.\n",
      "Content-Aware Adaptive Voxel Scooping Tracing\n",
      "FIGURE 5 | Comparative performances of ablation studies on a complex and noisy big neuron image to illustrate the role of CNN prediction and CAAT of the\n",
      "proposed method, by applying current novel enhancement methods [Jerman ﬁlter (Jerman et al., 2015) and CAEFI (Jeelani et al., 2019)] to replace CNN prediction\n",
      "(B) and tracing methods [FMST (Yang et al., 2019) and OpenSnake (Wang et al., 2011)] to replace CAAT (C) alternatively. Comparative tracing results of gold\n",
      "standard and our method (A). Inaccurate traced areas were marked by white ellipses.\n",
      "FIGURE 6 | Comparative performances of ablation studies on a discontinuous and noisy fMOST image to illustrate the role of CNN prediction and CAAT of the\n",
      "proposed method, by applying current novel enhancement methods [Jerman ﬁlter (Jerman et al., 2015) and CAEFI (Jeelani et al., 2019)] to replace CNN prediction\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms. The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of this study showed that the CAAT on CNN predicted probability maps was more effective in reducing high noise interferences and achieving more complete and accurate tracings of complex neurons than\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "(B) and tracing methods [FMST (Yang et al., 2019) and OpenSnake (Wang et al., 2011)] to replace CAAT (C) alternatively. Comparative tracing results of gold\n",
      "standard and our method (A). Inaccurate traced areas were marked by white ellipses.\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "8\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms such as FMST (Yang et al., 2019) and OpenSnake (Wang et al., 2011). The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of this study showed that the CAAT on CNN predicted probability maps was\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "Huang et al.\n",
      "Content-Aware Adaptive Voxel Scooping Tracing\n",
      "FIGURE 7 | Comparative performances of the proposed method and current novel algorithms on the public BigNeuron datasets. (A1–F4) Corresponding tracing\n",
      "results of different novel methods, our method and gold standard. Inaccurate traced areas were highlighted with yellow.\n",
      "TABLE 1 | Quantitative evaluation of the proposed and state-of-art methods on public BigNeuron datasets.\n",
      "Method\n",
      "Precision [%]\n",
      "Recall [%]\n",
      "Data\n",
      "Ours\n",
      "APP2\n",
      "neuTube\n",
      "ST-LVF\n",
      "CNN+APP2\n",
      "Ours\n",
      "APP2\n",
      "neuTube\n",
      "STL-VF\n",
      "CNN+APP2\n",
      "1\n",
      "98.9\n",
      "99.9\n",
      "98.9\n",
      "87.2\n",
      "99.4\n",
      "94.2\n",
      "74.0\n",
      "91.5\n",
      "92.5\n",
      "72.3\n",
      "2\n",
      "99.2\n",
      "50.0\n",
      "99.9\n",
      "92.0\n",
      "98.8\n",
      "97.9\n",
      "90.4\n",
      "94.3\n",
      "94.0\n",
      "85.6\n",
      "3\n",
      "98.3\n",
      "99.6\n",
      "99.3\n",
      "92.6\n",
      "100\n",
      "90.7\n",
      "61.0\n",
      "82.0\n",
      "84.4\n",
      "53.1\n",
      "4\n",
      "93.9\n",
      "98.7\n",
      "92.6\n",
      "49.9\n",
      "100\n",
      "92.5\n",
      "44.6\n",
      "76.6\n",
      "95.0\n",
      "29.8\n",
      "5\n",
      "98.1\n",
      "98.8\n",
      "98.7\n",
      "44.0\n",
      "99.6\n",
      "96.3\n",
      "79.4\n",
      "89.7\n",
      "90.1\n",
      "69.2\n",
      "6\n",
      "99.7\n",
      "100\n",
      "97.3\n",
      "87.7\n",
      "100\n",
      "96.0\n",
      "83.6\n",
      "95.5\n",
      "94.8\n",
      "84.7\n",
      "7\n",
      "99.4\n",
      "79.0\n",
      "52.2\n",
      "92.7\n",
      "95.9\n",
      "98.0\n",
      "94.7\n",
      "99.7\n",
      "99.1\n",
      "93.0\n",
      "8\n",
      "98.3\n",
      "100\n",
      "99.2\n",
      "87.7\n",
      "100\n",
      "95.5\n",
      "80.3\n",
      "91.2\n",
      "87.7\n",
      "72.9\n",
      "Average\n",
      "98.2\n",
      "90.8\n",
      "92.3\n",
      "79.2\n",
      "99.2\n",
      "95.1\n",
      "76.0\n",
      "90.1\n",
      "92.2\n",
      "70.1\n",
      "Std\n",
      "1.8\n",
      "18.0\n",
      "16.4\n",
      "20.1\n",
      "1.4\n",
      "2.6\n",
      "16.3\n",
      "7.5\n",
      "4.6\n",
      "20.4\n",
      "Bold values represent best achieved evaluations and gold standard.\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms such as FMST (Yang et al., 2019) and OpenSnake (Wang et al., 2011). The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of this study showed that the CAAT on CNN predicted probability maps was significantly\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "broken neurites. As shown in Table 2, the average precision and\n",
      "recall were 99.1% and 99.6% respectively for our method, which\n",
      "achieved approximately the same performance as the manual\n",
      "tracing and outperformed current novel algorithms (Xiao and\n",
      "Peng, 2013; Feng et al., 2015; Li et al., 2017a, 2019b) on broken\n",
      "neuron tracing from noisy background.\n",
      "We exhibited the performance of the proposed method and\n",
      "neuTube method, an automatic tracing algorithm that supports\n",
      "multi-neurite tracing, on some dense and challenging cases that\n",
      "contained dozens of neurites and various branch structures\n",
      "(Figure 9). The experiment was performed on six fMOST image\n",
      "stacks of size 1000 × 1000 × 300. Figure 9 showed two\n",
      "tracing examples, and some tracing results of separate neurites\n",
      "were magnified (Figures 9G–I). neuTube achieved good results\n",
      "on the image with high neurite signals and only missed few\n",
      "inhomogeneous neurites (Figure 9B), while it under-traced lots\n",
      "of dim and discontinuous neurites (Figure 9C). The proposed\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: \n",
      "\n",
      "A study published in Frontiers in Neuroanatomy on August 23rd, 2021, edited by Zoltan F. Kisvarday and reviewed by Tetsufumi Ito and Marcel Oberlaender, conducted by Qing Huang, Tingting Cao, Yijun Chen, Anan Li, Shaoqun Zeng, and Tingwei Quan from the Britton Chance Center for Biomedical Photonics at Huazhong University of Science and Technology, Wuhan, China, explores automated neuron tracing using a combination of content-aware adaptive voxel scooping (CAAT) on convolutional neural network (CNN) predicted probability maps and traditional algorithms such as FMST (Yang et al., 2019) and OpenSnake (Wang et al., 2011). The study employed deep convolutional neural networks (CNNs) to automatically extract more discriminative image features, with image resolutions between 0.1–0.593 µm × 0.1–0.593 µm × 0.3–2 µm and image sizes between areas, as well as a large-scale neuronal image of 3,630 × 3,630. The results of this study showed that the CAAT on CNN predicted probability maps was\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "method nearly traced all these neurites in these neuronal images\n",
      "with both high or dim or uneven neurite signals, and only missed\n",
      "one branch neurite (as pointed out in Figures 9C,H). It could\n",
      "capture most of these complex branch structures. Table 3 shows\n",
      "that the average precision was 96.4% and recall was 99.0% of the\n",
      "Frontiers in Neuroanatomy | www.frontiersin.org\n",
      "9\n",
      "August 2021 | Volume 15 | Article 712842\n",
      "------------\n",
      "Given the new context, refine the original summaryIf the context isn't useful, return the original summary.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlg_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\base.py:265\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    264\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    268\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\base.py:168\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 168\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    160\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    161\u001b[0m     inputs,\n\u001b[0;32m    162\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:56\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m     55\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m---> 56\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_docs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mother_keys)\n\u001b[0;32m     57\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\combine_documents\\refine.py:93\u001b[0m, in \u001b[0;36mRefineDocumentsChain.combine_docs\u001b[1;34m(self, docs, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m     base_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_refine_inputs(doc, res)\n\u001b[0;32m     92\u001b[0m     inputs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbase_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[1;32m---> 93\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefine_llm_chain\u001b[39m.\u001b[39mpredict(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m     94\u001b[0m     refine_steps\u001b[39m.\u001b[39mappend(res)\n\u001b[0;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(refine_steps, res)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\llm.py:155\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    142\u001b[0m     \u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\base.py:168\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 168\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    160\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    161\u001b[0m     inputs,\n\u001b[0;32m    162\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\llm.py:135\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    134\u001b[0m     known_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_inputs(inputs\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([known_values])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\llm.py:117\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m    116\u001b[0m     \u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[0;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\chains\\llm.py:59\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[1;32m---> 59\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[0;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\llms\\base.py:128\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 128\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\llms\\base.py:125\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    122\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m    123\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[0;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\llms\\openai.py:274\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    272\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39m, prompt\u001b[39m=\u001b[39m_prompts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    275\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[0;32m    277\u001b[0m     \u001b[39m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\llms\\openai.py:98\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\tenacity\\__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\tenacity\\__init__.py:406\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    405\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    408\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\tenacity\\__init__.py:351\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    349\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m retry_state\u001b[39m.\u001b[39moutcome\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(retry_state\u001b[39m.\u001b[39moutcome\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state\u001b[39m=\u001b[39mretry_state)):\n\u001b[1;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    354\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\tenacity\\__init__.py:409\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    408\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    410\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    411\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\langchain\\llms\\openai.py:96\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\openai\\api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[0;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\openai\\api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    517\u001b[0m         method,\n\u001b[0;32m    518\u001b[0m         abs_url,\n\u001b[0;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[0;32m    525\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\saha\\miniconda3\\envs\\working\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chain1.run(lg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa \n",
    "chain = load_qa_chain(llm,chain_type='map_rerank',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The main objective of this paper is to develop an automated neuron tracing method using content-aware adaptive voxel scooping on CNN predicted probability map.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('working')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11242012bb2bbaed9ab41e646dbb7d2cb06a6633762a7ee8ceab1e508cf89ab7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
